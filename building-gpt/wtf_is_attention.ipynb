{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The mathematical trick in self-attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 8, 2])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# toy example \n",
    "\n",
    "B,T,C = 4,8,2 # batch, time, channels or batch, time/tokens, dimentions \n",
    "\n",
    "# 4 independent input data, each with length of 8 tokens, and each token has 2 dims\n",
    "\n",
    "x = torch.randn(B,T,C)\n",
    "x.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8017,  1.1943],\n",
       "        [ 1.0161,  1.0499],\n",
       "        [ 0.7962,  0.5086],\n",
       "        [-0.6768, -1.9990],\n",
       "        [-0.2106, -0.1355],\n",
       "        [ 0.7507,  0.9637],\n",
       "        [-0.3740,  0.0230],\n",
       "        [ 1.7420, -0.2458]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]\n",
    "\n",
    "# first input vector, with 8 words / tokens , with each token having 2 dims \n",
    "# each row is sequencial, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- so we have 8 tokens, and what we want them to talk to each other,\n",
    "- here the way we want them to learn is from the past only, so in an input we have 8 tokens/time, they are sequencial \n",
    "- meaning, it's like a sentence with 8 words. \n",
    "- what we want is what word comes next is the function of it's past. \n",
    "- so if we are in the 5th word/token, we want it to be able see / talk to / learn from all the 4 tokens before it\n",
    "- but we dont want it to see the future, 6th, 7th, 8th token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# easiest way to communicate to past tokens ==> take avg of all the tokens all past tokens\n",
    "# extremely lossy way to aggregate past info, but it's a start \n",
    "\n",
    "xbow = torch.zeros((B,T,C))\n",
    "\n",
    "for b in range(B):\n",
    "    for t in range(T):\n",
    "        xprev = x[b, :t+1] # (t,C)\n",
    "        xbow[b,t] = torch.mean(xprev, 0 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8017,  1.1943],\n",
       "        [ 1.0161,  1.0499],\n",
       "        [ 0.7962,  0.5086],\n",
       "        [-0.6768, -1.9990],\n",
       "        [-0.2106, -0.1355],\n",
       "        [ 0.7507,  0.9637],\n",
       "        [-0.3740,  0.0230],\n",
       "        [ 1.7420, -0.2458]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.8017,  1.1943],\n",
       "        [ 0.1072,  1.1221],\n",
       "        [ 0.3369,  0.9176],\n",
       "        [ 0.0834,  0.1884],\n",
       "        [ 0.0246,  0.1237],\n",
       "        [ 0.1456,  0.2637],\n",
       "        [ 0.0714,  0.2293],\n",
       "        [ 0.2802,  0.1699]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xbow[0]\n",
    "\n",
    "# each row in the xbow metric is the avg of all the rows above it, including itslelf\n",
    "# in simple lang, its like moving avg\n",
    "# so the third row, is the third token/word, and now it has somehow information of all the words before it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Math trick to do this efficiently\n",
    "\n",
    "- basically,create such a matric (some wieght matric) that when we do a mat multiplication it with x mat the result is  the xbow mnatrix\n",
    "- for loops are super expensive and not at all efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 0., 0.],\n",
       "        [1., 1., 0.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tril(torch.ones(3,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 3])\n",
      "torch.Size([3, 1])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tril(torch.ones(3,3))\n",
    "a_sum = torch.sum(a, 1, keepdim=True)\n",
    "print(a.shape)\n",
    "print(a_sum.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = tensor([[1., 0., 0.],\n",
      "        [1., 1., 0.],\n",
      "        [1., 1., 1.]])\n",
      "a_sum = tensor([[1.],\n",
      "        [2.],\n",
      "        [3.]])\n",
      "a/a_sum = tensor([[1.0000, 0.0000, 0.0000],\n",
      "        [0.5000, 0.5000, 0.0000],\n",
      "        [0.3333, 0.3333, 0.3333]])\n"
     ]
    }
   ],
   "source": [
    "# why we need to keep the dim in a_sum \n",
    "# b/c when pytorch sees (3,3) is dividec by (3,1), it converts the (3,1) to (3,3) by duplicating the column and then it doesn and element wise division \n",
    "\n",
    "print(f\"a = {a}\")\n",
    "print(f\"a_sum = {a_sum}\")\n",
    "print(f\"a/a_sum = {a/a_sum}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b = tensor([[2., 7.],\n",
      "        [6., 4.],\n",
      "        [6., 5.]])\n",
      "below c metrix is a cumulative avg of all the rows above of mat b\n",
      "c = tensor([[2.0000, 7.0000],\n",
      "        [4.0000, 5.5000],\n",
      "        [4.6667, 5.3333]])\n"
     ]
    }
   ],
   "source": [
    "# exmaple\n",
    "\n",
    "torch.manual_seed(42)\n",
    "# a = some kinda weight metric\n",
    "a = torch.tril(torch.ones(3,3))\n",
    "a = a / torch.sum(a, 1, keepdim=True)\n",
    "# b = something like our x metric\n",
    "b = torch.randint(0,10,(3,2)).float()\n",
    "c = a @ b # dot multiplicaiton, essentially matrix multiplication\n",
    "print(f\"b = {b}\")\n",
    "print(\"below c metrix is a cumulative avg of all the rows above of mat b\")\n",
    "print(f\"c = {c}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Putting all together \n",
    "\n",
    "# our input info is in x mat, which (4,8,2) matrix; \n",
    "\n",
    "# weight matix \n",
    "wei = torch.tril(torch.ones(T,T))\n",
    "wei = wei / torch.sum(wei, 1, keepdim=True)\n",
    "\n",
    "xbow2 = wei @ x # (B,T,T) @ (B,T,C) ----> (B,T,C)  \n",
    "# for the wei metric, which is (T,T), when @ with x, the pytorch will add the dim B in front making it (B,T,T )\n",
    "# and pytorch will apply with multiplcation for each batch parellaly, essentilally doing (T,T) @ (T,C) for each of the batch element\n",
    "# pytorch is awesome\n",
    "\n",
    "torch.allclose(xbow,xbow2) # both are same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x's first input : \n",
      " tensor([[-0.8017,  1.1943],\n",
      "        [ 1.0161,  1.0499],\n",
      "        [ 0.7962,  0.5086],\n",
      "        [-0.6768, -1.9990],\n",
      "        [-0.2106, -0.1355],\n",
      "        [ 0.7507,  0.9637],\n",
      "        [-0.3740,  0.0230],\n",
      "        [ 1.7420, -0.2458]])\n",
      "xbow is essentially a cumulative avg of x for each row\n",
      "xbow's first input : \n",
      " tensor([[-0.8017,  1.1943],\n",
      "        [ 0.1072,  1.1221],\n",
      "        [ 0.3369,  0.9176],\n",
      "        [ 0.0834,  0.1884],\n",
      "        [ 0.0246,  0.1237],\n",
      "        [ 0.1456,  0.2637],\n",
      "        [ 0.0714,  0.2293],\n",
      "        [ 0.2802,  0.1699]])\n"
     ]
    }
   ],
   "source": [
    "# just reminding us what is xbow2 \n",
    "\n",
    "print(f\"x's first input : \\n {x[0]}\")\n",
    "print(\"xbow is essentially a cumulative avg of x for each row\")\n",
    "print(f\"xbow's first input : \\n {xbow[0]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Version 3: Use Softmax \n",
    "\n",
    "# building a wei matrix using softmax \n",
    "\n",
    "tril = torch.tril(torch.ones(T,T))\n",
    "# a simple weight matrix \n",
    "wei = torch.zeros(T,T) # how much weight we want to give to all the other tokens in T dim\n",
    "# wei = affinitiy between tokens and it will be data dependent, it will learn on which tockens from past to be given more/less weightage \n",
    "wei = wei.masked_fill(tril==0, float('-inf')) # dont look in future, only have a look in past, \n",
    "wei = F.softmax(wei, dim=1)\n",
    "xbow3 = wei @ x\n",
    "torch.allclose(xbow, xbow3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\n- dim 0 means across the rows, and 1 means across the columns \\n- so here, we think we want the softmax \"across the rows\", meaning take the first row, and then perform softmax on each of it\\'s element\\n- now, that is actually across the columns in pytorch language \\n- what is happening is that you want a softmax for the element of each of the columns in the first row\\n- hence, this is actually a operation column wise / across the columns, and so the dim would be 1 not 0 \\n'"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# a quick detour on how to use dim \n",
    "''' \n",
    "- dim 0 means across the rows, and 1 means across the columns \n",
    "- so here, we think we want the softmax \"across the rows\", meaning take the first row, and then perform softmax on each of it's element\n",
    "- now, that is actually across the columns in pytorch language \n",
    "- what is happening is that you want a softmax for the element of each of the columns in the first row\n",
    "- hence, this is actually a operation column wise / across the columns, and so the dim would be 1 not 0 \n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
